CS_DIR = /KM/ir/autocomplete/bin
XP_DIR = codebase/parser

DB = semantic-wikipedia-test
XML_FILE = $(DB).xml

all:

### HANNAH'S ESTER CHAIN (done 26Jul07, while at SIGIR in Amsterdam)
### 
### Just set XML_FILE to the desired file, and type "make all" :-)
###
### See below for how to build the index without entities

### Requires the following input from YAGO:
###
###   (all spaces in the example lines are tabs!)
###
### 1. <db>.words.translation -> contains lines of the form
###
###   ch:politician:leader_(the_person)		2363364	0	1
###   ch:pollutant:substance_(the_entity)	2363364	0	1
###
### 2. <db>.entity_frontier_pairs.rewritten -> contains lines of the form
###
###   ce:entity:angelamerkel:Angela_Merkel	ce:entity:angelamerkel:Angela_Merkel 
###   ce:entity:angelamerkel:Angela_Merkel	ce:femaletheperson:angelamerkel:Angela_Merkel 
###   ce:entity:angelamerkel:Angela_Merkel	ce:leadertheperson:angelamerkel:Angela_Merkel 
###   ce:entity:angelamerkel:Angela_Merkel	ce:objecttheentity:angelamerkel:Angela_Merkel 
###   ce:entity:angelamerkel:Angela_Merkel	ce:scientisttheperson:angelamerkel:Angela_Merkel 
###   ce:entity:angelamerkel:Angela_Merkel	ce:persontheorganism:angelamerkel:Angela_Merkel 
###
###   the one without rewritten also contains the --- useless --- lines of the form
###
###   ce:entity:angelamerkel:Angela_Merkel	angela::merkel:entity:Angela_Merkel
###
### 3a. yago.title_entity_pairs
###
###   Angela Merkel	entity:angelamerkel:Angela_Merkel
###   Exocrine gland	entity:exocrineglandthegland:exocrine_gland_(the_gland)
###   Guido Westerwelle	entity:guidowesterwelle:Guido_Westerwelle
###   Gabriele Zimmer	entity:gabrielezimmer:Gabriele_Zimmer
###
### 3b. yago.yago_id_wikipedia_url_pairs -> contains lines of the form
###   
###   41223	u:http://en.wikipedia.org/wiki/Angela%20Merkel
###   41224	u:http://en.wikipedia.org/wiki/Exocrine%20gland
###   41225	u:http://en.wikipedia.org/wiki/Guido%20Westerwelle
###   41226	u:http://en.wikipedia.org/wiki/Gabriele%20Zimmer
### 
### 4. yago/born_in_year.words etc. -> contains lines of the form
###
###   ce:entity:1952:1952	999231	0	1
###   ce:entity:1952:1952	999397	0	1
###   ce:entity:1953:1953	100204	0	1
###   ce:entity:1953:1953	1002718	0	1
###    

SECTIONS_TO_SKIP = semantic-wikipedia.sections_to_skip
EXTRACT_TITLES = semantic-wikipedia.extract_titles
YAGO_TITLE_ENTITY_PAIRS = yago.title_entity_pairs
PARSE = semantic-wikipedia.parse
TMP = /local/var/tmp
SORT = sort -T $(TMP)
TIME = /usr/bin/time -f "*** %E (user: %U, system: %S)"
LOG = $(DB).make-log
DOWNLOAD_URL = http://download.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2
HYB_PREFIX_LENGTH = 3


# 0. Download wikipedia xml
download:
	#if [ -f $(ORIGINAL_XML) ]; then echo "$(ORIGINAL_XML) exists, first move it out of the way ..."; exit 1; fi
	curl $(DOWNLOAD_URL) > wikipedia_en.xml.`date +"%d%b%y"`

# 1. Titles -> produces $(DB).title_entity_pairs
titles: $(XML_FILE) $(EXTRACT_TITLES) $(YAGO_TITLE_ENTITY_PAIRS).rewritten
	./$(EXTRACT_TITLES) $(XML_FILE) | $(SORT) -t "	" -k3,3 > $(DB).redirects
	join -t "	" -1 3 -2 1 $(DB).redirects $(YAGO_TITLE_ENTITY_PAIRS).rewritten -o "1.2 2.2" \
	  > $(DB).title_entity_pairs

# sort by titles, and rewrite entity:... to plain name (will be rewritten to entity by parser)
$(YAGO_TITLE_ENTITY_PAIRS).rewritten: $(YAGO_TITLE_ENTITY_PAIRS)
	$(SORT) -t "	" -k1,1 $(YAGO_TITLE_ENTITY_PAIRS) \
	  | perl -ne 's/([^\t]+)\tentity:[^:]+:([^:]+)/$$1\t$$2/; print;' > $(YAGO_TITLE_ENTITY_PAIRS).rewritten

# 2. Parse -> produces $(DB).words.original.sorted_by_doc $(DB).docs.original $(DB).vocabulary 
parse: ./$(PARSE)
	# -a means: output links and tag indexed words in $(DB).docs (so that we can enhance them later, for proper highlighting)
	# -L means: [[...]] around links in $(DB).docs (so that one can distinguish training data from what Alex's code learned)
	# DON'T WANT [[...]] CURRENTLY
	./$(PARSE) -a -s $(SECTIONS_TO_SKIP) -t $(DB).title_entity_pairs $(XML_FILE) \
	  $(DB).docs.original $(DB).words.original.sorted_by_doc $(DB).vocabulary.from_parser 

# 3. Sort -> produces $(DB).words.original
sort:
	$(SORT) -k1,1 -k2,2n -k4,4n $(DB).words.original.sorted_by_doc > $(DB).words.original

# 4. Relations -> produces $(DB).words.relations
relations: $(DB).yago_id_wikipedia_id_pairs yago.words.relations.sorted_by_id
	join -1 2 -2 1 yago.words.relations.sorted_by_id $< -o "1.1 2.2 1.3 1.4" \
	  | $(SORT) -k1,1 -k2,2n -k4,4n \
	  | perl -ne 's/ /\t/g;print' \
	  > $(DB).words.relations

yago.words.relations.sorted_by_id: yago.words.relations
	$(SORT) -t "	" -k2,2 $< > $@

yago.yago_id_wikipedia_title_pairs: yago.yago_id_wikipedia_url_pairs
	cat $^ \
	  | perl -ne '($$x,$$y)=/(\d+)\tu:http:\/\/en.wikipedia.org\/wiki\/(.*)/;$$y=~s/%20/ /g;print"$$x\t$$y\n";' \
	  | $(SORT) -t "	" -k2,2 \
	  > $@

$(DB).yago_id_wikipedia_id_pairs: yago.yago_id_wikipedia_title_pairs $(DB).redirects
	$(SORT) -t "	" -k2,2 $(DB).redirects \
	  | join -t "	" -1 2 -2 2 $< - -o "1.1 2.1" \
	  | $(SORT) -k1,1 \
	  > $@

# 5. Frontier set entities -> produces $(DB).words.frontier.[12]
frontier: wikipedia_en.entity_frontier_pairs.rewritten
	join -1 1 -2 1 $(DB).words.original $< -o "2.2 1.2 1.3 1.4" \
	  | $(SORT) -k1,1 -k2,2n -k4,4n \
	  | perl -ne 's/ /\t/g;print' \
	  > $(DB).words.frontier.1
	join -1 1 -2 1 $(DB).words.relations $< -o "2.2 1.2 1.3 1.4" \
	  | $(SORT) -k1,1 -k2,2n -k4,4n \
	  | perl -ne 's/ /\t/g;print' \
	  > $(DB).words.frontier.2

wikipedia_en.entity_frontier_pairs.rewritten: wikipedia_en.entity_frontier_pairs
	grep "	ce:" $^ > $@
	#  | grep -v "	ce:entity:" 

# 6. Translation -> produces $(DB).words.translation
translation: wikipedia_en.words.translation
	cp $< $(DB).words.translation

# 7. Merge -> produces $(DB). words
merge:
	$(SORT) -m -k1,1 -k2,2n -k4,4n \
	  $(DB).words.original \
		$(DB).words.relations \
		$(DB).words.frontier.? \
		$(DB).words.translation \
	  | grep "^[[:print:]]\+	" \
	  > $(DB).words

	  #| grep "^[!-~À-ÿ]\+	" \

# 8. Index -> produces $(DB).hybrid and $(DB).vocabulary
index:
	cut -f1 $(DB).words | $(SORT) -u > $(DB).vocabulary
	cat $(DB).vocabulary | grep -v ":" | cut -c1-$(HYB_PREFIX_LENGTH) | $(SORT) -u > $(DB).hybrid.prefixes.1
	#cut -f1 $(DB).vocabulary.from_parser | $(SORT) | cut -c1-2 | $(SORT) -u > $(DB).hybrid.prefixes.1
	grep ":" $(DB).vocabulary > $(DB).vocabulary.colon_words
	cat $(DB).vocabulary.colon_words | grep "^cr:" | cut -c1-3 | $(SORT) -u > $(DB).hybrid.prefixes.2
	cat $(DB).vocabulary.colon_words | grep "^ch:" | cut -c1-5 | $(SORT) -u > $(DB).hybrid.prefixes.3
	cat $(DB).vocabulary.colon_words | grep "^cn:" | cut -c1-7 | $(SORT) -u > $(DB).hybrid.prefixes.4
	cat $(DB).vocabulary.colon_words | grep "^ce:entity:" | cut -c1-14 | $(SORT) -u > $(DB).hybrid.prefixes.5
	cat $(DB).vocabulary.colon_words | grep "^c[et]:" | grep -v "^ce:entity:" \
	                                 | perl -ne '/^(c.:[^:]+):/ and print "$$1:\n";' \
																	 | $(SORT) -u > $(DB).hybrid.prefixes.6
	$(SORT) -m $(DB).hybrid.prefixes.* > $(DB).hybrid.prefixes
	rm -f $(DB).hybrid.prefixes.* 
	$(CS_DIR)/buildIndex HYB $(DB).words -b $(DB).hybrid.prefixes \
	  2>.$(DB).hybrid.build-index-errors | tee .$(DB).hybrid.build-index-log

# 9. Excerpts -> produces $(DB).docs and $(DB).docs.db
docs: $(DB).words.enhancing 
	$(MAKE) -C bin enhanceExcerpts
	bin/enhanceExcerpts $(DB).docs.original $(DB).docs $<
	#rm -rf $(DB).docs.db
	#$(CS_DIR)/ExcerptsToDB $(DB).docs $(DB).docs.db
	sort -c -k1,1n $(DB).docs
	bin/buildDocsDB -f $(DB).docs

$(DB).words.enhancing: $(DB).words.original.sorted_by_doc
	# NOTE: the -u in the sort is crucial, because enhanceExcerpts behaves erratically if there are duplicates
	grep "^ce:entity:" $< \
	  | perl -ne 's/ce:entity:([^:]+):([^\t]+)/cexentityx^^$$1/ and print unless /\t0$$/;' \
	  | sort -k2,2n -k4,4n -u \
	  > $@

# 10. geek2
geek2: 
	scp $(DB).hybrid bast@geek2:/var/opt/completesearch/databases/$(DB).hybrid
	scp $(DB).vocabulary bast@geek2:/var/opt/completesearch/databases/$(DB).vocabulary
	scp $(DB).docs.DB bast@geek2:/var/opt/completesearch/databases/$(DB).docs.DB
	ssh bast@geek2 "make -C /var/opt/completesearch/databases $(DB).start </dev/null"

# ALL
all:
	rm -f $(LOG); touch $(LOG)
	printf "\n*** 1. TITLES\n"      2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) titles          2>&1 | tee -a $(LOG)
	printf "\n*** 2. PARSE\n"       2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) parse           2>&1 | tee -a $(LOG)
	printf "\n*** 3. SORT\n"        2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) sort            2>&1 | tee -a $(LOG)
	printf "\n*** 4. RELATIONS\n"   2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) relations       2>&1 | tee -a $(LOG)
	printf "\n*** 5. FRONTIER\n"    2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) frontier        2>&1 | tee -a $(LOG)
	printf "\n*** 6. TRANSLATION\n" 2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) translation     2>&1 | tee -a $(LOG)
	printf "\n*** 7. MERGE\n"       2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) merge           2>&1 | tee -a $(LOG)
	printf "\n*** 8. INDEX\n"       2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) index           2>&1 | tee -a $(LOG)
	printf "\n*** 9. DOCS\n"        2>&1 | tee -a $(LOG)
	$(TIME) $(MAKE) docs            2>&1 | tee -a $(LOG)
	printf "\n\nSUMMARY\n\n"        2>&1 | tee -a $(LOG)
	cat $(LOG) | grep "^\*\*\*"     2>&1 | tee -a $(LOG)


# 0. CLEAN

clean:
	rm -f $(DB).docs.original
	rm -f $(DB).words.original.sorted_by_doc


pack:
	tar -cf $(DB).tar $(DB).hybrid $(DB).docs.DB $(DB).vocabulary
	bzip2 $(DB).tar


$(PARSE): $(PARSE).cpp $(XP_DIR)/XmlParser.cpp
	g++ -O6 -Wall -DVERSION="\"`date +"Version %d%b%y %H:%M"`\"" -I$(XP_DIR) -o $@ $^ -lexpat
	#g++ -g -Wall -DVERSION="\"`date +"Version %d%b%y %H:%M"`\"" -I$(XP_DIR) -o $@ $^ -lexpat

$(EXTRACT_TITLES): $(EXTRACT_TITLES).cpp $(XP_DIR)/XmlParser.cpp
	g++ -O6 -Wall -DVERSION="\"`date +"Version %d%b%y %H:%M"`\"" -I$(XP_DIR) -o $@ $^ -lexpat
	#g++ -g -Wall -DVERSION="\"`date +"Version %d%b%y %H:%M"`\"" -I$(XP_DIR) -o $@ $^ -lexpat

extract:
	$(EXTRACT_TITLES) $(XML_FILE) | sort > test.titles


### BUILD ORDINARY WIKIPEDIA INDEX (without entities)

ordinary:
	$(PARSE) $(XML_FILE) $(DB).docs $(DB).words.sorted_by_doc $(DB).vocabulary



### TESTING

test-hybridBlocks2: test-hybridBlocks2.cpp $(XP_DIR)/XmlParser.cpp
		g++ -O6 -Wall -DVERSION="\"`date +"Version %d%b%y %H:%M"`\"" -I$(XP_DIR) -o $@ $^ -lexpat


### START SERVER

%.geek: %.hybrid %.vocabulary %.docs.db
	scp $*.hybrid bast@geek:/var/opt/autocomplete/databases/
	scp $*.vocabulary bast@geek:/var/opt/autocomplete/databases/
	scp $*.docs.db bast@geek:/var/opt/autocomplete/databases/
	ssh bast@geek "make -C /var/opt/autocomplete/databases $*.server </dev/null"

%.geek-test: %.hybrid %.vocabulary %.docs.db
	scp $*.hybrid bast@geek:/var/opt/autocomplete/databases/test-holger.hybrid
	scp $*.vocabulary bast@geek:/var/opt/autocomplete/databases/test-holger.vocabulary
	scp $*.docs.db bast@geek:/var/opt/autocomplete/databases/test-holger.docs.db
	ssh bast@geek "make -C /var/opt/autocomplete/databases test-holger.server </dev/null"


### INDEX BUILDING

#%.words: %.words.sorted_by_doc %.words.enhancing
%.words: %.words.sorted_by_doc 
	sort -b -k1,1 -k2,2n -k4,4n $^ > $@

%.hybrid %.vocabulary: %.words 
	$(CS_DIR)/buildIndex HYB $*.words -b 3
	  2>.$*.hybrid.build-index-errors | tee .$*.hybrid.build-index-log

%.docs.db: %.docs
	rm -rf $*.docs.db
	$(CS_DIR)/ExcerptsToDB $*.docs $*.docs.db



### ENHANCING EXCERPTS (still testing)

#test.docs.enhanced: test.docs.original test.words.enhancing
#	time bin/enhanceExcerpts test.docs.original $@ test.words.enhancing

#test.words.enhancing: test.words.sorted_by_doc
#	grep "^ce:entity:" test.words.sorted_by_doc \
#	  | perl -ne 's/ce:entity:([^:]+):([^\t]+)/cexentityx^^$$1/ and print unless /\t0$$/;' | sort -k2,2n -k4,4n > $@
#	#grep "bert" test.words.sorted_by_doc | perl -ne 's/^[^\t]+/xxx/ and print unless /^linkforalex:/;' | sort -k2,2n -k4,4n > $@

enhance:
	rm -f test.words.enhancing
	rm -f test.docs.enhanced
	$(MAKE) test.docs.enhanced
	ls -ltr $(XML_FILE) test.docs.original test.docs.enhanced

alex-enhance:
	rm -f test.docs.enhanced
	cat alex-twit/test.words.wsd-entities \
	  | perl -ne 's/ce:entity:([^:]+):([^\t]+)/cexentityx^^$$1/ and print unless /\t0$$/;' > test.words.enhancing
	time bin/enhanceExcerpts test.docs.original test.docs.enhanced test.words.enhancing
	ls -ltr $(XML_FILE) test.docs.original test.docs.enhanced
	sort -b -k1,1 -k2,2n -k4,4n test.words.sorted_by_doc alex-twit/test.words.wsd-entities > test.words

alex-cp: 
	cp -f test.titles alex
	cp -f test.words.sorted_by_doc alex
	cp -f test.vocabulary alex
	ln -sf $(XML_FILE) alex/test.xml

### YAGO STUFF

yago_dir_OLD:
	perl -ne 's/^bornInYear:/ce:entity:/;print' yago_OLD/bornInYear_wikipedia_bornInYear1.txt.txt > yago/born_in_year.words
	perl -ne 's/^diedInYear:/ce:entity:/;print' yago_OLD/diedInYear_wikipedia_diedInYear1.txt.txt > yago/died_in_year.words
	perl -ne 's/^writtenInYear:/ce:entity:/;print' yago_OLD/writtenInYear_wikipedia_writtenInYear1.txt.txt > yago/written_in_year.words
	perl -ne 's/^locatedIn:/ce:entity:/;print' yago_OLD/locatedIn_wikipedia_locatedIn1.txt.txt > yago/located_in.words
	perl -ne 's/^hasWonPrize:/ce:entity:/;print' yago_OLD/hasWonPrize_wikipedia_hasWonPrize1.txt.txt > yago/has_won_prize.words
	perl -ne 's/^type:/ce:class:/;print' yago_OLD/type_typeextractor.txt.txt > yago/is_a.words

yago_dir: Y = yago_FABIAN
yago_dir:
	for F in born_in_year died_in_year written_in_year established_in_year located_in has_won_prize politician_of; do \
	  perl -ne 's/^cr:/ce:entity:/;print' $(Y)/$$F.words | sed '/.../!d' | sort -u > yago/$$F.words; done
	perl -ne 's/^ce:entity:/ce:class:/;print' $(Y)/is_a.words | sed '/.../!d' | sort -u > yago/is_a.words

yago_relations:
	perl yago.words.relations.pl
	sort -b -k1,1 -k2,2n -k4,4n yago/*.ce.words yago/*.cr.words > yago.words.relations

wikipedia_en.ce\:_counts:
	cat ingmar_dork/wikipedia_en.words.entities \
	  | perl -ne '/^(ce:[^:]+:)/ and print "$$1\n";' \
	  | uniq -c > wikipedia_en.ce:_counts

### OLDER YAGO STUFF (Ingmar is doing these things now)

# all special words from Fabian's ontology
wikipedia_en.words.yago:
	time cat yago/*.txt | perl -ne 's/1$$/0/;print lc($$_);' | sort -b -k1,1 -k2,2n -k4,4n -u > $@

# code for extracting given titles from the whole xml
wikipedia_en.extract: wikipedia_en.extract.cpp $(XP_DIR)/XmlParser.cpp
		g++ -O3 -Wall -I$(XP_DIR) -o $@ $^ -lexpat

# make docs and words (including yago words) for all titles starting with % plus pages linked from these pages
yago_part.%:
	cut -f3 wikipedia_en.docs | cut -d: -f2 | grep -i "^$*$$" | sort -u > wikipedia_en.titles_matching_$*
	wc -l wikipedia_en.titles_matching_$*
	wikipedia_en.extract wikipedia_en.xml wikipedia_en.titles_matching_$* > wikipedia_en.xml.titles_matching_$*
	wikipediaXmlParser.pl en wikipedia_en.xml.titles_matching_$*
	grep "^entity:" ZZwikipedia_en.words.unsorted \
	  | perl -ne '($$x)=/^entity:[^:]+:([^	]+)/;$$x=~s/_/ /g;print "$$x\n";' \
	  | sort -u > wikipedia_en.titles_matching_$*_plus_links \
	  > wikipedia_en.titles_matching_$*_plus_links
	wc -l wikipedia_en.titles_matching_$*_plus_links
	wikipedia_en.extract wikipedia_en.xml wikipedia_en.titles_matching_$*_plus_links > wikipedia_en.xml.titles_matching_$*_plus_links
	wikipediaXmlParser.pl en wikipedia_en.xml.titles_matching_$*_plus_links
	sort -k2,2 ZZwikipedia_en.docs > yago_part.docs_sorted_by_url
	rm -f ZZwikipedia_en.docs
	mv ZZwikipedia_en.words.unsorted yago_part.words_unsorted

wikipedia_en.docids_and_urls_sorted_by_urls: wikipedia_en.docs
	cut -f1,2 wikipedia_en.docs | sort -k2,2 > wikipedia_en.docids_and_urls_sorted_by_url

# map the doc ids to the original ones (otherwise incompatible with yago words) % = yago_part
%.map: %.docs_sorted_by_url %.words_unsorted
	join -t"	" -1 2 -2 2 -o "1.1 2.1" $*.docs_sorted_by_url wikipedia_en.docids_and_urls_sorted_by_url \
	  | perl -ne 's/ /\t/;print;' \
	  | sort -k1,1 > $*.docid_map
	sort -k1,1 $*.docs_sorted_by_url \
	  | join -t"	" -1 1 -2 1 -o "2.2 1.2 1.3 1.4" - $*.docid_map \
	  | sort -k1,1 > $*.docs
	sort -k2,2 wikipedia_en.words.yago \
	  | join -t"	" -1 2 -2 1 -o "1.1 1.2 1.3 1.4" - $*.docs \
	  | sort -b -k1,1 -k2,2n -k4,4n > wikipedia_en.words.yago_part
	sort -k2,2 $*.words_unsorted \
	  | join -t"	" -1 2 -2 1 -o "1.1 2.2 1.3 1.4" - $*.docid_map \
	  | sort -b -k1,1 -k2,2n -k4,4n \
	  | sort -m -k1,1 -k2,2n -k4,4n - wikipedia_en.words.yago_part > $*.words



### OLDER STUFF

wikipedia_en.words.entities_1:
	cat /KM/ir-data/ingmar/wikipedia/XXwikipedia_en.words.unsorted.split \
	  | grep "	[a-zA-Z][^	]*$$" \
	  | perl -ne 'chomp;@f=split(/\t/,$$_); \
	              $$x=lc($$f[4]);$$x=~s/[\W_]//g;$$y=$$f[4];$$y=~s/ /_/g; \
		      print "entity:$$x:$$y\t$$f[1]\t$$f[2]\t$$f[3]\n";' \
	  | sort -k1,1 -k2,2n -k4,4n > $@

wikipedia_en.words.entities_2:
	cut -f1,3 wikipedia_en.docs \
	  | perl -ne 'chomp;($$x,$$y)=split(/\t/,$$_);$$y=~s/^t://; \
	              $$z=lc($$y);$$z=~s/[\W_]//g;$$y=~s/ /_/g; \
		      print "entity:$$z:$$y\t$$x\t2\t0\n";' \
	  | sort -k1,1 -k2,2n -k4,4n > $@

#wikipedia_en.words: wikipedia_en.words.original wikipedia_en.words.entities_1 wikipedia_en.words.entities_2 wikipedia_en.words.yago
wikipedia_en.words: wikipedia_en.words.original wikipedia_en.words.entities wikipedia_en.words.yago
	sort -m -k1,1 -k2,2n -k4,4n $^ > $@
