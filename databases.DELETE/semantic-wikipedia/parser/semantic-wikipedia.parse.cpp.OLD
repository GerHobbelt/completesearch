#include <stdio.h>
#include <string>
#include <ext/hash_map>
#include <ext/hash_set>
#include "codebase/parser/XmlParser.h"

using namespace __gnu_cxx;

extern bool XP_REPLACE_WHITESPACE;

const char* CS_CATEGORY_PREFIX = "ct:";
const char* CS_ENTITY_PREFIX = "ce:";
const char* CS_ROTATION_PREFIX = NULL;  // Don't add rotations of special words to index.

FILE* entities_file;
typedef hash_map<string, unsigned int, HashString> HM; HM vocabulary;
hash_map<string, string, HashString> titles;
hash_set<string, HashString> sectionHeadersToSkip;
bool outputLinks = false;
bool prefixLinks = false;
bool keepTagsInExcerpts = false;
bool tagIndexedWordsInExcerpts = false;
bool keepParanthesesInExcerpts = false;
bool buildVocabulary = true;
string titlesFileName = "";
string sectionHeadersToSkipFileName = "";
unsigned int titleScore = 2;
unsigned int categoryScore = 0;
unsigned int linkScore = 0;
unsigned int minWordLength = 2;
#define IDX_TAG ''

#define MY_MIN(a,b) ((a) < (b) ? (a) : (b))

//! SUBCLASS PARSER (nothing happening here)
class WikipediaParser : public XmlParser 
{ 
  // overload this method from XmlParser
  void outputDocAndWords();
};

//! WIKIPEDIA SPECIFIC IS_WORD_CHAR (copied from XmlParser.h and modified)
//#define W_IS_BEGIN_LINK(p) (IS_WORD_CHAR[*(p)] == '[' && *(p) == *((p)+1))
//#define W_IS_END_LINK(p) (IS_WORD_CHAR[*(p)] == ']' && *(p) == *((p)+1))
const char W_CHAR_MAP[257] =
// xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\]^_`abcdefghijklmnopqrstuvwxyz{|}~x
// xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx¿¡¬√ƒ≈∆«»… ÀÃÕŒœ–—“”‘’÷◊ÿŸ⁄€‹›ﬁﬂ‡·‚„‰ÂÊÁËÈÍÎÏÌÓÔÒÚÛÙıˆ˜¯˘˙˚¸˝˛ˇ
  "sssssssssssssssssssssssssssssssss000000F00000000wwwwwwwwww00<F>00wwwwwwwwwwwwwwwwwwwwwwwwww[0]000wwwwwwwwwwwwwwwwwwwwwwwwww[|]00"
  "wwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww";


//! OVERLOAD OUTPUT METHOD (all the application-specific parsing is in here)
void WikipediaParser::outputDocAndWords()
{
  // ONLY <page>...</page> items (there is also <siteinfo>...</siteinfo>)
  if (strcmp(getItemName(), "page") != 0) 
  {
    fprintf(_log_file, "SKIPPED ITEM with name \"%s\"\n", getItemName());
    return;
  }

  ostringstream os;
  static const unsigned int MAX_BUF_SIZE = 5000;
  static unsigned char* buf = (unsigned char*)malloc(MAX_BUF_SIZE);
  //static unsigned char* buf2 = (unsigned char*)malloc(MAX_BUF_SIZE);
  static const unsigned int MAX_TEXT_LEN = 1000*1000;
  static const unsigned int MAX_EXCERPT_LEN = 2*MAX_TEXT_LEN + 2;
  // there is +1 for a the leading space (see below), and +1 for a trailing zero (not used currently)
  static unsigned char* excerpt = (unsigned char*)malloc(MAX_EXCERPT_LEN);
  static hash_map<string, unsigned int, HashString> wordsInCurrentDoc;

  // 1. GET ID, TITLE, TEXT
  vector<const char*> ids = getItems("id");
  if (ids.size() < 2) { cerr << "FATAL ERROR: page does not have two id tags" << endl; exit(1); }
  const char* id = ids[0];
  const char* title = getItem("title", "");
  if (*title == 0) fprintf(_log_file, "NO TITLE: in page with id %s\n", id);
  const char* text = getItem("text", "");
  if (*text == 0) fprintf(_log_file, "NO TEXT: in page with id %s\n", id);

  // 1a. SKIP PAGES TITLED Wikipedia:...
  if (strncmp(title, "Wikipedia:", 10) == 0) 
  {
    fprintf(_log_file, "SKIPPED SPECIAL PAGE with title \"%s\"\n", title);
    return;
  }

  // 1b. SKIP REDIRECT PAGES 
  if (strncmp(text, "#REDIRECT ", 10) == 0) 
  {
    fprintf(_log_file, "SKIPPED REDIRECT PAGE with text \"%s\"\n", text);
    return;
  }

  // !!! BEWARE: make sure to consistently use id as the document id in the following !!!
  _docId = atoi(id); assert(_docId != 0);

  // 2a. OUTPUT FIRST PART OF DOCS LINE (everything without excerpt)
  unsigned int title_len = strlen(title); 
  if (title_len > MAX_BUF_SIZE - 1)
  {
    fprintf(_log_file, "TITLE TRUNCATED: in page with id %s; full title is \"%s\"\n", id, title);
    title_len = MAX_BUF_SIZE - 1;
  }
  // replace whitespace by _
  for (unsigned int i = 0; i < title_len; ++i)
    buf[i] = isspace(title[i]) ? '_': title[i];
  buf[title_len] = 0;
  // output 
  fprintf(_docs_file, "%s\tu:http://en.wikipedia.org/wiki/%s\tt:%s\tH:", id , buf, title);

  // 2b. WRITE TITLE TO WORDS FILE (as, for example, entity:alberteinstein:Albert_Einstein)
  if (titles.count(title) > 0)
    outputWords(titles[title], "entity", CS_ENTITY_PREFIX, CS_ROTATION_PREFIX, _docId, titleScore, 0);
    //outputWords(title, "entity", "ce:", "", _docId, 64, 0);
    //unsigned char* p = buf2;
    //unsigned char c;
    //for (unsigned int i = 0; i < title_len; ++i)
    //  if (isalnum(c = title[i])) *p++ = tolower(c);
    //*p = 0;
    //fprintf(_words_file, "entity:%s:%s\t%s\t2\t0\n", buf2, buf, id);

  // 2c. ALSO WRITE WORDS OF TITLE (otherwise "The Beatles" not ranked top for query "beatles", because of "cn:" above)
  outputWords(title, _docId, titleScore);


  // 3. PARSE TEXT 
  //
  //   incrementally build excerpt and output lines of words file in one pass !
  //
  //   NOTE: excerpt must never be longer than original text (see malloc above)
  //
  unsigned int text_len = strlen(text); 
  if (strlen(text) > MAX_TEXT_LEN) 
  {
    fprintf(_log_file, "TEXT TRUNCATED: in page with id %s and title \"%s\"\n", id, title);
    text_len = MAX_TEXT_LEN; 
  }
  unsigned char* t = (unsigned char*)(text);
  unsigned char* t_end = t + text_len;
  unsigned char* e = (unsigned char*)(excerpt);
  *e++ = ' '; // start with a space; ip this allows us to always reference *(e-1)
  unsigned char* e_0 = e; // position just after last non-word character
  int link_level = 0; // just for checking whether every [[ is matched by a ]] 
  unsigned int len;
  bool insideLink = false;
  string sectionHeader;
  bool skip = false;
  wordsInCurrentDoc.clear();
  _pos = 1;
  while (t < t_end)
  {
    // CASE: word character
    if ( W_CHAR_MAP[*t] == 'w' )
    {
      *e++ = *t++;
      continue;
    }

    // CASE: word ended (no "continue" here: must still deal with current character)
    if (W_CHAR_MAP[*(e-1)] == 'w' && (len = e - e_0) >= minWordLength && *(e_0-1) != '<' && !skip) 
    { 
      if (len > MAX_BUF_SIZE - 1) len = MAX_BUF_SIZE - 1;
      for (unsigned int i = 0; i < len; ++i) buf[i] = tolower(e_0[i]);
      buf[len] = 0;
      fprintf(_words_file, "%s\t%s\t%d\t%d\n", buf, id, 0, _pos);
      if (buildVocabulary && ++wordsInCurrentDoc[(char*)(buf)] == 1) vocabulary[(char*)(buf)]++;
        //fprintf(_words_file, "%-25s\t%d\t%s\t%d\n", buf, id, 0, _pos);
      if (!insideLink)
      {
        if (tagIndexedWordsInExcerpts) *e++ = IDX_TAG;
        ++_pos;
      }
    }

    // CASE: notice when a section starts
    if (*t == '=')
    {
      if (t == (unsigned char*)(text) || *(t-1) == '\n')
      {
        t++; 
        while (t < t_end && *t == '=') t++;
        while (t < t_end && *t == ' ') t++;
        if (t >= t_end) break;
        unsigned char* t_0 = t;
        while (t < t_end && *t != '=') t++;
        while (*(t-1) == ' ' && t > t_0) t--;
        sectionHeader.assign((const char*)(t_0), t - t_0);
        skip = (sectionHeadersToSkip.count(sectionHeader) > 0);
          //if (skip == true) cout << " [SKIP SECTION HEADER: " << sectionHeader << "] " << flush;
        t = t_0 - 1;
      }
      //else { sectionHeader = ""; }
    }

    // CASE: discard formatting-only characters like ' and = (marked with F in W_CHAR_MAP above)
    if (W_CHAR_MAP[*t] == 'F' && *t == *(t+1))
    {
      unsigned char c = *t;
      do { t++; } while (t < t_end && *t == c);
      e_0 = e; 
      continue;
    }

    // CASE: comment (skip them)
    if (*t == '<' && strncmp((char*)(t+1), "!--", 3) == 0)
    { 
      t += 4;
      while (t < t_end)
        if (*t == '-' && *(t+1) == '-' && *(t+2) == '>') { t += 3; break; } else { t++; }
      e_0 = e; 
      continue;
    }

    // CASE: math (skip it and write "[math]" to excerpts)
    if (*t == '<' && strncmp((char*)(t+1), "math>", 5) == 0)
    {
      t += 6;
      while (t < t_end)
        if (*t == '<' && strncmp((char*)(t+1), "/math>", 6) == 0) { t += 7; break; } else { t++; }
      strncpy((char*)(e), "[math]", 6);
      e += 6;
      e_0 = e; 
      continue;
    }

    // CASE: other tag -> ignore tag (optionally) but leave the contents
    if (!keepTagsInExcerpts && *t == '<')
    { 
      while (t < t_end && *t != '>') t++;
      t++;
      e_0 = e; 
      continue;
    }

    // CASE: link begin
    if (W_CHAR_MAP[*t] == '[' && *t == *(t+1)) 
    {
      // CHECK: can strncmp go beyond end of text here? (if text is null-terminated, it would be ok)
      
      // CASE: stop parsing this page after this link (links to same page in other languages)
      if (strncmp((char*)(t), "{{Link ",7) == 0) break;
      
      // CASE: skip these special Wikipedia constructs (we neither want them in the index nor in the excerpts)
      if (*t == '{' && *(t+1) == '{')
      {
        if (strncasecmp((char*)(t), "{{NPOV", 6) == 0 ||
            strncasecmp((char*)(t), "{{wiktionary", 12) == 0 ||
            strncasecmp((char*)(t), "{{wikiquote", 11) == 0 ||
            strncasecmp((char*)(t), "{{sprotected2", 13) == 0 ||
            strncasecmp((char*)(t), "{{otheruses", 11) == 0 ||
            strncasecmp((char*)(t), "{{Infobox", 9) == 0 ||
	    strncasecmp((char*)(t), "{{Taxobox", 9) == 0 ||
	    strncasecmp((char*)(t), "{{Redirect", 10) == 0 ||
	    strncasecmp((char*)(t), "{{Politics", 10) == 0 ||
	    strncasecmp((char*)(t), "{{cleanup", 9) == 0 ||
	    strncasecmp((char*)(t), "{{unreferenced", 14) == 0 ||
	    strncasecmp((char*)(t), "{{dablink", 9) == 0 ||
	    strncasecmp((char*)(t), "{{for", 5) == 0 ||
	    strncasecmp((char*)(t), "{{Guitarist", 11) == 0)
	{
	  t += 2;
	  unsigned int par_level = 1;
	  while (t < t_end && par_level > 0)
	  {
	    while (t < t_end && *t != '{' && *t != '}') t++;
	    if (t < t_end && *t == '{' && *(t+1) == '{') { t++; par_level++; }
	    if (t < t_end && *t == '}' && *(t+1) == '}') { t++; par_level--; }
	    if (t < t_end) t++;
	  }
	  continue;
	}
      }
      
      // CASE: skip images
      if (strncasecmp((char*)(t), "[[Image:", 8) == 0)
      {
        while (t < t_end && *t != ']') t++;
        while (t < t_end && *t == ']') t++;
	continue;
      }

      // CASE: category link (add special words with permutations + remove from excerpt)
      if (strncmp((char*)(t), "[[Category:", 11) == 0) 
      { 
        t += 11;
        while (t < t_end && W_CHAR_MAP[*t] == 's') t++;
        unsigned char* t_0 = t;
        // Also permit [[Category ended by newline instead of by ]]
	// Only until first |
        while (t < t_end && *t != ']' && *t != '\n' && *t != '|') t++;
          //unsigned char m;
          //while (t < t_end && (m = W_CHAR_MAP[*t], m == 'w' || m == 's')) t++;
        string s((const char*)(t_0), t - t_0);
        // put category words + permutations at position 0
        outputWords(s, "category", CS_CATEGORY_PREFIX, CS_ROTATION_PREFIX, _docId, categoryScore, 0);
        while (t < t_end && *t != ']' && *t != '\n') t++;
        while (t < t_end && *t == ']') t++;
        e_0 = e;
        continue;
      }

      // DEFAULT: discard everything before a |
      if (keepParanthesesInExcerpts) { *e++ = *t++; *e++ = *t++; } else { t += 2; }
      unsigned char* t_0 = t;
      unsigned char* t_1 = t;
      char m = 0;
      while ( t < t_end && (m = W_CHAR_MAP[*t], m != '[' && m != ']') )
      {
        t++;
        if (m == '|') t_1 = t;
      }
      string s((const char*)(t_0), t_0 == t_1 ? t - t_0 : t_1 - t_0 - 1);
      if (outputLinks && m == ']' && titles.count(s) > 0 && !skip)
      {
        string anchor((const char*)(t_1), t - t_1);
        for (unsigned int i = 0; i < anchor.size(); i++) 
          anchor[i] = isspace(anchor[i]) ? '_' : tolower(anchor[i]);
        if (buildVocabulary && ++wordsInCurrentDoc[anchor] == 1) vocabulary[anchor]++;
        outputWords(titles[s], "entity", CS_ENTITY_PREFIX, CS_ROTATION_PREFIX, _docId, linkScore, _pos);
        //outputWords(s, "entity", "ce:", "", _docId, 2, _pos);
        //fprintf(_words_file, "ce:%s\t%s\t2\t%d\n", titles[s].c_str(), id, _pos);
        insideLink = true;
      }
      t = t_1;
      link_level++;
      //if (W_CHAR_MAP[*(e-1)] == 'w') *e++ = ' '; // separate from previous word for cases like [[...]][[...]]
      e_0 = e; 
      continue;
    }

    // CASE: link end
    if (link_level > 0 && W_CHAR_MAP[*t] == ']' && *t == *(t+1)) 
    { 
      *e++ = IDX_TAG; 
      insideLink = false;
      _pos++;

      if (keepParanthesesInExcerpts) { *e++ = *t++; *e++ = *t++; } else { t += 2; }
      link_level--;

      // TODO: special words for links 
      //
      // e.g. antifa|resistance to neo-Nazi groups
      //
      //  normal   -> entity:antifabianthexyz:Anti_Fabian_(the_Xyz) <TAB> doc id <TAB> flag <TAB> pos (which pos?) 
      //  
      //  for Alex -> resistance to neo nazi groups <TAB> doc id <TAB> flag <TAB> pos <TAB> entity:antifabianthexyz:Anti_Fabian_(the_Xyz)
      //
      //  assuming that the Wikipedia page "antifa" is the entity "Anti Fabian" in Yago

/*
      // add special word for link, e.g., link:Albert_Einstein:Anchor_text
      // NOTE: for nested links will only do this for the innermost link (which is the desired behaviour)
      if (l != NULL && (unsigned int)(t - l) < MAX_BUF_SIZE)
      {
        unsigned char* p;
        unsigned char c;
        p = buf; while (l < t) { *p++ = isspace(c = *l++) ? '_' : c; } 
        //p = buf2; while (l_2 < e) { *p++ = isspace(c = *l_2++) ? '_' : c; }
        *p = 0;
        if (outputLinks) fprintf(_words_file, "linkforalex:%s\t%s\t2\t%d\n", buf, id, l_pos);
        l = NULL;
      }
*/     
      e_0 = e; 
      continue;
    }

    // CASE: if inside of link, add ^^^ between words
    if (insideLink) 
    {
      if (*(e-1) != '^') e += sprintf((char*)e, "^^^");
      //if (W_CHAR_MAP[*t] != 's') *e++ = *t;
      t++;
      e_0 = e; 
      continue;
    }

    // CASE: replace each sequence of whitespace by one single space [BEWARE: using s flag in W_CHAR_MAP instead of isspace]
    if (W_CHAR_MAP[*t] == 's')
    {
      if (*(e-1) != ' ') *e++ = ' ';
      t++;
      e_0 = e; 
      continue;
    }

    // DEFAULT: just copy character (can only be non-word at this point)
    {
      *e++ = *t++;
      e_0 = e;
    }

  }
  if (link_level != 0) fprintf(_log_file, "UNMATCHED [[ or {{ in document with id %s (link_level = %d)\n", id, link_level);
  *e++ = '\n';
  assert((unsigned int)(e - excerpt) <= MAX_EXCERPT_LEN);

  // now output the excerpt
  fwrite(excerpt, 1, e - excerpt, _docs_file);

}  


//! MAIN 
int main(int argc, char** argv)
{   
  while (true)
  {
    int c = getopt(argc, argv, "aLm:t:s:T");
    if (c == -1) break;
    switch (c)
    {
      case 'a': outputLinks = true; tagIndexedWordsInExcerpts = true; break;
      case 'm': minWordLength = atoi(optarg); break;
      case 't': titlesFileName = optarg; break;
      case 's': sectionHeadersToSkipFileName = optarg; break;
      case 'L': keepParanthesesInExcerpts = true; break;
      case 'V': buildVocabulary = false; break;
      case 'T': keepTagsInExcerpts = true; break;
      default : cout << endl << "! ERROR in processing options (getopt returned '" 
                     << c << "' = 0x" << setbase(16) << int(c) << ")" << endl << endl; exit(1); 
    }
  }
  string xmlFileName = optind < argc ? argv[optind++] : "wikipedia_en.xml.TEST";
  string docsFileName = optind < argc ? argv[optind++] : "test.docs.original"; //wikipedia_en.docs"; 
  string wordsFileName = optind < argc ? argv[optind++] : "test.words.original.sorted_by_doc"; 
  string vocabularyFileName = optind < argc ? argv[optind++] : "test.vocabulary"; 
  string entitiesFileName = optind < argc ? argv[optind++] : "test.words.entities"; 
  string logFileName = string(argc > 0 ? argv[0] : "parser") + ".LOG";

  cerr << endl << EMPH_ON << "WIKIPEDIA PARSER (" << VERSION << ")" << EMPH_OFF << endl << endl;

  // open entities file for writing (other files are opened in xml parser)
  entities_file = fopen(entitiesFileName.c_str(), "w");
  if (entities_file == NULL) { perror(entitiesFileName.c_str()); exit(1); }

  // read entity names of titles from file
  if (titlesFileName.size() > 0)
  {
    FILE* file = fopen(titlesFileName.c_str(), "r");
    if (file == NULL) { perror(titlesFileName.c_str()); exit(1); }
    const unsigned int MAX_LINE_SIZE = 1000;
    char* line = (char*)malloc(MAX_LINE_SIZE+2);
    line[MAX_LINE_SIZE] = '\t';
    line[MAX_LINE_SIZE+1] = '\n';
    char* p;
    while (true)
    {
      char* ret = fgets(line, MAX_LINE_SIZE, file);
      if (ret == NULL) { assert(feof(file)); break; }
      p = line; 
      while(*p != '\t') { p++; } assert(p < line + MAX_LINE_SIZE); 
      *p++ = 0; 
      char* p_0 = p;
      while(*p != '\n') { p++; } assert(p < line + MAX_LINE_SIZE); 
      *p++ = 0;
      titles[line] = p_0;
    }
    fclose(file);
    cerr << "read " << titles.size() << " titles from file \"" << titlesFileName << "\"" << endl << endl;
  }

  // read section headers to skip
  if (sectionHeadersToSkipFileName.size() > 0)
  {
    FILE* file = fopen(sectionHeadersToSkipFileName.c_str(), "r");
    if (file == NULL) { perror(sectionHeadersToSkipFileName.c_str()); exit(1); }
    const unsigned int MAX_LINE_SIZE = 1000;
    char* line = (char*)malloc(MAX_LINE_SIZE+1);
    while (true)
    {
      char* ret = fgets(line, MAX_LINE_SIZE+1, file);
      if (ret == NULL) { assert(feof(file)); break; }
      unsigned int len = strlen(line);
      if (len == 0) continue;
      line[len - 1] = 0;
      sectionHeadersToSkip.insert(line);
    }
    fclose(file);
    cerr << "read " << sectionHeadersToSkip.size() << " section headers from file \"" 
         << sectionHeadersToSkipFileName << "\"" << endl << endl;
  }


  // TODO: parser currently loops forever when xml file is empty!!!

  // parse xml
  WikipediaParser xp;
  XP_REPLACE_WHITESPACE = false;
  xp.parse(xmlFileName, docsFileName, wordsFileName, logFileName);

  // show statistics and log
  xp.showStatistics();
  xp.showLog();

  // write vocabulary
  if (buildVocabulary)
  {
    FILE* vocabulary_file = fopen(vocabularyFileName.c_str(), "w");
    if (vocabulary_file == NULL) { perror(vocabularyFileName.c_str()); exit(1); }
    for (HM::iterator it = vocabulary.begin(); it != vocabulary.end(); ++it)
      fprintf(vocabulary_file, "%s\t%d\n", it->first.c_str(), it->second);
    fclose(vocabulary_file);
  }
}
